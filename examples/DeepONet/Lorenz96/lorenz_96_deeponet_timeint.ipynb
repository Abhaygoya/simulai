{
    "cells": [
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# (C) Copyright IBM Corp. 2019, 2020, 2021, 2022.\n",
                "\n",
                "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
                "#    you may not use this file except in compliance with the License.\n",
                "#    You may obtain a copy of the License at\n",
                "\n",
                "#           http://www.apache.org/licenses/LICENSE-2.0\n",
                "\n",
                "#     Unless required by applicable law or agreed to in writing, software\n",
                "#     distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                "#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                "#     See the License for the specific language governing permissions and\n",
                "#     limitations under the License.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "25142fcf-1535-4513-82af-f7d8dc62f092",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from unittest import TestCase\n",
                "import os\n",
                "\n",
                "# In order to execute this script, it is necessary to\n",
                "# set the environment variable engine as \"pytorch\" before initializing\n",
                "# simulai\n",
                "os.environ['engine'] = 'pytorch'\n",
                "\n",
                "from simulai.utilities.lorenz_solver import lorenz_solver\n",
                "from simulai.regression import DenseNetwork\n",
                "from simulai.models import ResDeepONet as DeepONet\n",
                "from simulai.optimization import Optimizer\n",
                "from simulai.metrics import L2Norm\n",
                "from simulai.io import IntersectingBatches"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8f5ec229-eab3-4492-932d-00c77fb1e819",
            "metadata": {},
            "outputs": [],
            "source": [
                "def project_to_interval(interval, data):\n",
                "\n",
                "    return interval[1]*(data - data.min())/(data.max() - data.min()) + interval[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b077ab4d-b0e2-4a75-acc2-78bc98e74487",
            "metadata": {},
            "outputs": [],
            "source": [
                "dt = 0.0025\n",
                "T_max = 50\n",
                "rho = 28\n",
                "beta = 8 / 3\n",
                "beta_str = '8/3'\n",
                "sigma = 10\n",
                "\n",
                "initial_state = np.array([1, 0, 0])[None, :]\n",
                "lorenz_data, derivative_lorenz_data, time = lorenz_solver(rho=rho, dt=dt, T=T_max, sigma=sigma,\n",
                "                                                          initial_state=initial_state,\n",
                "                                                          beta=beta, beta_str=beta_str,\n",
                "                                                          data_path='/tmp', solver='RK45')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1c814968-9e22-4d6b-b167-8e48e07883ea",
            "metadata": {},
            "outputs": [],
            "source": [
                "# The fraction of data used for training the model.\n",
                "train_fraction = 0.9\n",
                "delta_t = 0.25 # in seconds\n",
                "batching = 'segmented' #'intersected'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d5712525-607f-45ce-86e6-b2975281aca6",
            "metadata": {},
            "outputs": [],
            "source": [
                "if batching == 'segmented':\n",
                "\n",
                "    time_chunks_ = [time[(time >= i) & (time <= i + delta_t)] for i in np.arange(0, T_max, delta_t)]\n",
                "    data_chunks_ = [lorenz_data[(time >= i) & (time <= i + delta_t)] for i in np.arange(0, T_max, delta_t)]\n",
                "\n",
                "    n_batches = len(time_chunks_)\n",
                "    n_batches_train = int(train_fraction * n_batches)\n",
                "\n",
                "    _time_chunks = [chunk[:] for chunk in time_chunks_]\n",
                "    data_chunks = [chunk[:] for chunk in data_chunks_]\n",
                "\n",
                "    initial_states = [chunk[0] for chunk in data_chunks_]\n",
                "\n",
                "    time_chunks = [project_to_interval([0, delta_t], chunk)[:, None] for chunk in _time_chunks]\n",
                "\n",
                "    time_chunks_train = time_chunks[:n_batches_train]\n",
                "    data_chunks_train = data_chunks[:n_batches_train]\n",
                "    initial_states_train = initial_states[:n_batches_train]\n",
                "\n",
                "    time_chunks_test = time_chunks[n_batches_train:]\n",
                "    data_chunks_test = data_chunks[n_batches_train:]\n",
                "    initial_states_test = initial_states[n_batches_train:]\n",
                "\n",
                "elif batching == 'intersected':\n",
                "\n",
                "    n_samples = lorenz_data.shape[0]\n",
                "    n_samples_train = int(train_fraction*n_samples)\n",
                "\n",
                "    batcher = IntersectingBatches(skip_size=1, batch_size=int(delta_t/dt))\n",
                "\n",
                "    time_chunks_ = batcher(input_data=time[:n_samples_train])\n",
                "    data_chunks = batcher(input_data=lorenz_data[:n_samples_train])\n",
                "\n",
                "    T_max_train = n_samples_train*dt\n",
                "\n",
                "    time_aux = [time[(time >= i) & (time <= i + delta_t)] for i in np.arange(T_max_train, T_max, delta_t)]\n",
                "    data_aux = [lorenz_data[(time >= i) & (time <= i + delta_t)] for i in np.arange(T_max_train, T_max, delta_t)]\n",
                "\n",
                "    initial_states = [chunk[0] for chunk in data_chunks]\n",
                "\n",
                "    time_chunks = [project_to_interval([0, delta_t], chunk)[:, None] for chunk in time_chunks_]\n",
                "\n",
                "    time_chunks_train = time_chunks\n",
                "    data_chunks_train = data_chunks\n",
                "    initial_states_train = initial_states\n",
                "\n",
                "    time_chunks_test = [project_to_interval([0, delta_t], chunk)[:, None] for chunk in time_aux]\n",
                "    data_chunks_test = data_aux\n",
                "    initial_states_test = [chunk[0] for chunk in data_aux]\n",
                "\n",
                "else:\n",
                "\n",
                "    raise Exception(f\"The option {batching} for batching does not exist.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "953a7a9a-ce82-4b18-a022-430a52bc9007",
            "metadata": {},
            "outputs": [],
            "source": [
                "branch_input_train = np.vstack([np.tile(init, (time_chunk.shape[0], 1))\n",
                "                                for init, time_chunk in zip(initial_states_train, time_chunks_train)])\n",
                "\n",
                "branch_input_test = np.vstack([np.tile(init, (time_chunk.shape[0], 1))\n",
                "                               for init, time_chunk in zip(initial_states_test, time_chunks_test)])\n",
                "\n",
                "trunk_input_train = np.vstack(time_chunks_train)\n",
                "trunk_input_test = np.vstack(time_chunks_test)\n",
                "\n",
                "output_train = np.vstack(data_chunks_train)\n",
                "output_test = np.vstack(data_chunks_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "94097d01-a4eb-45b6-b7b6-59d93c03db1b",
            "metadata": {},
            "outputs": [],
            "source": [
                "input_labels = ['x', 'y', 'z']\n",
                "output_labels = ['x_dot', 'y_dot', 'z_dot']\n",
                "\n",
                "n_inputs = len(input_labels)\n",
                "n_outputs = len(output_labels)\n",
                "\n",
                "lambda_1 = 0.0  # Penalty for the L\u00b9 regularization (Lasso)\n",
                "lambda_2 = 1e-5  # Penalty factor for the L\u00b2 regularization\n",
                "n_epochs = 2_000  # Maximum number of iterations for ADAM\n",
                "lr = 1e-3  # Initial learning rate for the ADAM algorithm\n",
                "n_latent = 100\n",
                "\n",
                "# Configuration for the fully-connected trunk network\n",
                "trunk_config = {\n",
                "                'layers_units': 3*[100],  # Hidden layers\n",
                "                'activations': 'tanh',\n",
                "                'input_size': 1,\n",
                "                'output_size': n_latent*n_outputs,\n",
                "                'name': 'trunk_net'\n",
                "               }\n",
                "\n",
                "# Configuration for the fully-connected branch network\n",
                "branch_config = {\n",
                "                'layers_units': 3*[100],  # Hidden layers\n",
                "                'activations': 'tanh',\n",
                "                'input_size': n_inputs,\n",
                "                'output_size': n_latent*n_outputs,\n",
                "                'name': 'branch_net',\n",
                "                }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9a318d0c-e680-4dc2-addf-5587c10b3bd4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Instantiating and training the surrogate model\n",
                "trunk_net = DenseNetwork(**trunk_config)\n",
                "branch_net = DenseNetwork(**branch_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a317b02f-a7e0-4a11-8fc5-6e753d1742ba",
            "metadata": {},
            "outputs": [],
            "source": [
                "optimizer_config = {'lr': lr}\n",
                "\n",
                "# Maximum derivative magnitudes to be used as loss weights\n",
                "maximum_values = (1/np.linalg.norm(output_train, 2, axis=0)).tolist()\n",
                "\n",
                "params = {'lambda_1': lambda_1, 'lambda_2': lambda_2, 'weights': maximum_values}\n",
                "\n",
                "# It prints a summary of the network features\n",
                "trunk_net.summary()\n",
                "branch_net.summary()\n",
                "\n",
                "input_data = {'input_branch': branch_input_train, 'input_trunk': trunk_input_train}\n",
                "\n",
                "lorenz_net = DeepONet(trunk_network=trunk_net,\n",
                "                      branch_network=branch_net,\n",
                "                      var_dim=n_outputs,\n",
                "                      model_id='lorenz_net')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8a188f6c-b77e-444a-8739-40f97d6e8bf9",
            "metadata": {},
            "outputs": [],
            "source": [
                "optimizer = Optimizer('adam', params=optimizer_config)\n",
                "\n",
                "optimizer.fit(op=lorenz_net, input_data=input_data, target_data=output_train,\n",
                "              n_epochs=n_epochs, loss=\"wrmse\", params=params, device='gpu')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "211680a2-6468-431a-8c6b-c50d8e7f4779",
            "metadata": {},
            "outputs": [],
            "source": [
                "approximated_data = lorenz_net.eval(trunk_data=trunk_input_test, branch_data=branch_input_test)\n",
                "\n",
                "l2_norm = L2Norm()\n",
                "\n",
                "error = 100*l2_norm(data=approximated_data, reference_data=output_test, relative_norm=True)\n",
                "\n",
                "for ii in range(n_inputs):\n",
                "\n",
                "    plt.plot(approximated_data[:, ii], label=\"Approximated\")\n",
                "    plt.plot(output_test[:, ii], label=\"Exact\")\n",
                "    plt.legend()\n",
                "    plt.savefig(f'lorenz_deeponet_time_int_{ii}.png')\n",
                "    plt.show()\n",
                "\n",
                "print(f\"Approximation error for the variables: {error} %\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}